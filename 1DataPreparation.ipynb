{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <center> Exploiting Sentinel-1 imagery time series to detect grasslands in northern Brazil tropical plains</center> </h1>\n",
    "<h3> <center> Part 1 - Data preparation </center> </h3>\n",
    "<center> Arian Ferreira Carneiro </center>\n",
    "<center>Willian Vieira de Oliveira </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "[Dicas Sidnei] Para justificar o trabalho:\n",
    "\n",
    "- buscar imagens ópticas\n",
    "- mostrar as limitações dessas imagens e a importância do uso de imagens de radar na região"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Websites that might be useful: </b>\n",
    "\n",
    "https://gee-python-api.readthedocs.io/en/latest/ee.html\n",
    "\n",
    "   - Tips to work with large datasets in Pandas:\n",
    "   \n",
    "    https://towardsdatascience.com/why-and-how-to-use-pandas-with-large-data-9594dda2ea4c\n",
    "\n",
    "\n",
    "   - Converting from javascript to python api\n",
    "   \n",
    "    https://gis.stackexchange.com/questions/336080/converting-map-from-javascript-api-to-python-api\n",
    "    \n",
    "    https://github.com/GreenInfo-Network/earthengine-prototyping/issues/6\n",
    "    \n",
    "    \n",
    "   - GEE data in Pandas\n",
    "    \n",
    "    https://mygeoblog.com/2017/01/13/your-gee-data-in-pandas/\n",
    "    \n",
    "    https://mygeoblog.com/2017/10/06/from-gee-to-numpy-to-geotiff/\n",
    "    \n",
    "    \n",
    "   - Others:\n",
    "    \n",
    "    https://www.linkedin.com/pulse/cloud-computing-land-cover-classification-part-1-how-jo%C3%A3o-otavio/\n",
    "    \n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from osgeo import gdal, gdal_array, ogr\n",
    "#from osgeo import osr\n",
    "#import matplotlib.pyplot as plt\n",
    "#import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input parameters\n",
    "\n",
    "### Image data cubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories for the image cubes\n",
    "dir_NL = \"DATA/ee_export/NL.tif\"\n",
    "dir_Ratio = \"DATA/ee_export/Ratio.tif\"\n",
    "dir_RGI = \"DATA/ee_export/RGI.tif\"\n",
    "dir_VH = \"DATA/ee_export/VH.tif\"\n",
    "dir_VV = \"DATA/ee_export/VV.tif\"\n",
    "\n",
    "dir_datacubes = [dir_NL, dir_Ratio, dir_RGI, dir_VH, dir_VV]\n",
    "\n",
    "dir_output = \"OUTPUT/\"\n",
    "filenames = ['NL', 'Ratio', 'RGI', 'VH', 'VV']\n",
    "\n",
    "columns = ['2017-09-22', '2017-10-04','2017-10-16','2017-10-28','2017-11-09','2017-11-21','2017-12-03','2017-12-15',\n",
    "           '2017-12-27','2018-01-08','2018-01-20','2018-02-01','2018-02-13','2018-02-25','2018-03-09','2018-03-21',\n",
    "           '2018-04-02','2018-04-14','20 18-04-26','2018-05-08','2018-05-20','2018-06-01','2018-06-13','2018-06-25',\n",
    "           '2018-07-07','2018-07-19','2018-07-31','2018-08-12','2018-08-24','2018-09-05','2018-09-17']\n",
    "\n",
    "# Would you like to write the dataframes to CSV files? ['YES', 'NO']\n",
    "#write_CSV = 'NO'\n",
    "write_CSV = 'YES'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shapefiles with point locations of samples of different classes\n",
    "shp_pasto = \"DATA/ee_export/Biomas/2_pasto/8_centroid/pPasto.shp\"\n",
    "shp_floresta = \"DATA/ee_export/Biomas/3_floresta/8_centroid/centroid_fl.shp\"\n",
    "shp_agricultura = \"DATA/ee_export/Biomas/4_agricultura/8_centroid/cent_agri.shp\"\n",
    "\n",
    "dir_samples = [shp_pasto, shp_floresta, shp_agricultura]\n",
    "filenames_samples = ['Pasto', 'Floresta', 'Agricultura']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure 1 - Generation of a dataframe that describes the time series of each pixel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained a dataframe regarding each one the image data cubes. \n",
    "\n",
    "First of all, we removed border pixels (3-pixel wide) around the data cubes due to the presence of null pixels. We read the data cubes as n-dimensional arrays. Then, we converted these arrays to flatten arrays, where each line represents a pixel and each column presents the value observed in each band of the time series. Finally, we converted this array to dataframe and wrote it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "\n",
    "def openImage(filepath):\n",
    "    data = gdal.Open(filepath)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file:  DATA/ee_export/NL.tif\n",
      "The dataframe was written to file!\n",
      "\n",
      "\n",
      "Processing file:  DATA/ee_export/Ratio.tif\n",
      "The dataframe was written to file!\n",
      "\n",
      "\n",
      "Processing file:  DATA/ee_export/RGI.tif\n",
      "The dataframe was written to file!\n",
      "\n",
      "\n",
      "Processing file:  DATA/ee_export/VH.tif\n",
      "The dataframe was written to file!\n",
      "\n",
      "\n",
      "Processing file:  DATA/ee_export/VV.tif\n",
      "The dataframe was written to file!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_id = 0\n",
    "df_datacubes = []\n",
    "\n",
    "for filepath in dir_datacubes:\n",
    "    print(\"Processing file: \", filepath)\n",
    "    datacube = openImage(filepath)\n",
    "\n",
    "    Nrows = datacube.RasterYSize - 6 # We do not consider border pixels. We removed both the first and the last three rows.\n",
    "    Ncols = datacube.RasterXSize - 6 # We do not consider border pixels. We removed both the first and the last three columns.\n",
    "    Nbands = datacube.RasterCount\n",
    "    \n",
    "    arr = datacube.ReadAsArray(3, 3, Ncols, Nrows) # xoff, yoff, xcount, ycount\n",
    "    #print(arr.shape)\n",
    "    \n",
    "    df_list = []\n",
    "    for band in range(0, Nbands):\n",
    "        array = arr[band].flatten()\n",
    "        df = pd.DataFrame(array, columns=[columns[band]])\n",
    "        df_list.append(df)\n",
    "       \n",
    "    df_datacube = pd.concat(df_list, axis=1)    \n",
    "    \n",
    "    # Writing the dataframe to CSV file\n",
    "    if (write_CSV == 'YES'):\n",
    "        filename = dir_output+filenames[file_id]+'.csv'\n",
    "        try:\n",
    "            #df_datacube.to_csv(filename, sep=',', index=False, encoding='utf-8')\n",
    "            df_datacube.to_csv(filename, sep=',', index=False, encoding='utf-8-sig') # using 'utf-8-sig' encoding \n",
    "                                                                            #improves efficiency to open it on Excel.\n",
    "            print(\"The dataframe was written to file!\")\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "    \n",
    "    df_datacubes.append(df_datacube)\n",
    "    \n",
    "    # Closing file\n",
    "    datacube = None\n",
    "    \n",
    "    file_id = file_id + 1\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure 2 - Generation of metrics for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the dataframes previously obtained in order to computed different metrics. We computed metrics for each pixel using the values observed in their respective time series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "\n",
    "def ComputeMetrics(df):\n",
    "    # Header of the new dataframe\n",
    "    metrics_header = np.array(['Mean', 'Std', 'Sum', 'Min', 'Max', 'Amplitude', 'CoefVariation', 'FirstSlope'])\n",
    "    \n",
    "    # Dataframe, composed only by the header\n",
    "    df_metrics = pd.DataFrame(columns=metrics_header)\n",
    "    \n",
    "    # Metrics\n",
    "    df_metrics['Mean'] = df.apply(lambda row : row.mean(), axis = 1)\n",
    "    df_metrics['Std'] = df.apply(lambda row : row.std(), axis = 1)\n",
    "    df_metrics['Sum'] = df.apply(lambda row : row.sum(), axis = 1)\n",
    "    df_metrics['Min'] = df.apply(lambda row : row.min(), axis = 1)\n",
    "    df_metrics['Max'] = df.apply(lambda row : row.max(), axis = 1)\n",
    "    df_metrics['Amplitude'] = df.apply(lambda row : row.max()-row.min(), axis = 1)\n",
    "    df_metrics['CoefVariation'] = df.apply(lambda row : row.std()/row.mean(), axis = 1)\n",
    "    df_metrics['FirstSlope'] = df.apply(lambda row : np.max(abs(np.diff(row))), axis = 1) # First slope maximum\n",
    "        \n",
    "    return df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing metrics for:  NL\n",
      "The dataframe was written to file!\n",
      "\n",
      "\n",
      "Processing metrics for:  Ratio\n",
      "The dataframe was written to file!\n",
      "\n",
      "\n",
      "Processing metrics for:  RGI\n",
      "The dataframe was written to file!\n",
      "\n",
      "\n",
      "Processing metrics for:  VH\n",
      "The dataframe was written to file!\n",
      "\n",
      "\n",
      "Processing metrics for:  VV\n",
      "The dataframe was written to file!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_id = 0\n",
    "list_df_metrics = []\n",
    "\n",
    "for df in df_datacubes:\n",
    "    print(\"Processing metrics for: \", filenames[file_id])\n",
    "    df_metrics = ComputeMetrics(df)\n",
    "    \n",
    "    # Writing the dataframe to CSV file\n",
    "    if (write_CSV == 'YES'):\n",
    "        filename = dir_output+filenames[file_id]+'_metrics'+'.csv'\n",
    "        try:\n",
    "            #df_metrics.to_csv(filename, sep=',', index=False, encoding='utf-8')\n",
    "            df_metrics.to_csv(filename, sep=',', index=False, encoding='utf-8-sig') # using 'utf-8-sig' encoding \n",
    "                                                                            #improves efficiency to open it on Excel.\n",
    "            print(\"The dataframe was written to file!\")\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "    \n",
    "    list_df_metrics.append(df_metrics)\n",
    "    \n",
    "    file_id = file_id + 1\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure 3 - Definition of sample sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the function 'ExtractSamples' in order to extract the time series of the pixels identified by the point locations described in the shapefile archives. We performed this procedure to each one of the data cubes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractSamples(raster, shp):\n",
    "    \n",
    "    #Define header\n",
    "    #header=['X', 'Y'] # In case we desire to include the coordinates in the dataframe\n",
    "    header_dates = ['2017-09-22', '2017-10-04','2017-10-16','2017-10-28','2017-11-09','2017-11-21','2017-12-03','2017-12-15',\n",
    "           '2017-12-27','2018-01-08','2018-01-20','2018-02-01','2018-02-13','2018-02-25','2018-03-09','2018-03-21',\n",
    "           '2018-04-02','2018-04-14','2018-04-26','2018-05-08','2018-05-20','2018-06-01','2018-06-13','2018-06-25',\n",
    "           '2018-07-07','2018-07-19','2018-07-31','2018-08-12','2018-08-24','2018-09-05','2018-09-17']\n",
    "    \n",
    "    #create dataframe\n",
    "    #df = pd.DataFrame(columns=header+header_dates)\n",
    "    df = pd.DataFrame(columns=header_dates)\n",
    "    \n",
    "    #df_temp = pd.DataFrame(np.nan, index=range(1), columns=header+header_dates)\n",
    "    df_temp = pd.DataFrame(np.nan, index=range(1), columns=header_dates)\n",
    "    \n",
    "    src_ds = gdal.Open(raster)\n",
    "    gt = src_ds.GetGeoTransform()\n",
    "    \n",
    "    Nbands = src_ds.RasterCount\n",
    "      \n",
    "    ds = ogr.Open(shp)\n",
    "    lyr = ds.GetLayer()\n",
    "    \n",
    "    row = 0\n",
    "    for feat in lyr:\n",
    "        geom = feat.GetGeometryRef()\n",
    "        mx, my = geom.GetX(), geom.GetY() # coord in map units\n",
    "        \n",
    "        # Convert from map to pixel coordinates\n",
    "        # Only works for geotransforms with no rotation\n",
    "        px = int((mx - gt[0]) / gt[1]) # x pixel\n",
    "        py = int((my - gt[3]) / gt[5]) # y pixel\n",
    "        \n",
    "        #df_temp['X'].loc[0] = float(mx)\n",
    "        #df_temp['Y'].loc[0] = float(my)\n",
    "        \n",
    "        for band in range(0, Nbands):\n",
    "            rb = src_ds.GetRasterBand(band+1)\n",
    "            intval = rb.ReadAsArray(px, py, 1, 1)\n",
    "            \n",
    "            df_temp[header_dates[band]].loc[0] = float(intval[0]) #### this is the value of the pixel, forcing it to a float \n",
    "        \n",
    "        df.loc[row] = df_temp.loc[0]\n",
    "        row = row + 1\n",
    "        \n",
    "    # Closing files\n",
    "    src_ds = None\n",
    "    ds = None\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the function previouly generated to extract pixel values from several point locations in order to compose sample sets related to the classes analysed in this study. In this procedure, we obtain a dataframe that describes the samples related to each one of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining samples for raster:  NL\n",
      "  Class:  Pasto\n",
      "    The dataframe was written to file!\n",
      "  Class:  Floresta\n",
      "    The dataframe was written to file!\n",
      "  Class:  Agricultura\n",
      "    The dataframe was written to file!\n",
      "\n",
      "\n",
      "Defining samples for raster:  Ratio\n",
      "  Class:  Pasto\n",
      "    The dataframe was written to file!\n",
      "  Class:  Floresta\n",
      "    The dataframe was written to file!\n",
      "  Class:  Agricultura\n",
      "    The dataframe was written to file!\n",
      "\n",
      "\n",
      "Defining samples for raster:  RGI\n",
      "  Class:  Pasto\n",
      "    The dataframe was written to file!\n",
      "  Class:  Floresta\n",
      "    The dataframe was written to file!\n",
      "  Class:  Agricultura\n",
      "    The dataframe was written to file!\n",
      "\n",
      "\n",
      "Defining samples for raster:  VH\n",
      "  Class:  Pasto\n",
      "    The dataframe was written to file!\n",
      "  Class:  Floresta\n",
      "    The dataframe was written to file!\n",
      "  Class:  Agricultura\n",
      "    The dataframe was written to file!\n",
      "\n",
      "\n",
      "Defining samples for raster:  VV\n",
      "  Class:  Pasto\n",
      "    The dataframe was written to file!\n",
      "  Class:  Floresta\n",
      "    The dataframe was written to file!\n",
      "  Class:  Agricultura\n",
      "    The dataframe was written to file!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list_df_samples = []\n",
    "\n",
    "raster_id = 0\n",
    "for raster in dir_datacubes:\n",
    "    file_id = 0\n",
    "    print(\"Defining samples for raster: \", filenames[raster_id])\n",
    "    \n",
    "    for shp in dir_samples:\n",
    "        print(\"  Class: \", filenames_samples[file_id])\n",
    "        \n",
    "        # Extracting sample values\n",
    "        df = ExtractSamples(raster, shp)\n",
    "        \n",
    "        # Writing the dataframe to file\n",
    "        if (write_CSV == 'YES'):\n",
    "            filename = dir_output+filenames[raster_id]+'_Samples_'+filenames_samples[file_id]+'.csv'\n",
    "        \n",
    "            try:\n",
    "                #df.to_csv(filename, sep=',', index=False, encoding='utf-8')\n",
    "                df.to_csv(filename, sep=',', index=False, encoding='utf-8-sig') # using 'utf-8-sig' encoding \n",
    "                                                                                #improves efficiency to open it on Excel.\n",
    "                print(\"    The dataframe was written to file!\")\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "\n",
    "        list_df_samples.append(df)\n",
    "        \n",
    "        file_id = file_id + 1\n",
    "    raster_id = raster_id + 1\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting metrics for all samples sets\n",
    "After we obtained the sample sets, we computed the same metrics obtained for the data cubes (Procedure 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing metrics for:  NL\n",
      "  Class:  Pasto\n",
      "The dataframe was written to file!\n",
      "  Class:  Floresta\n",
      "The dataframe was written to file!\n",
      "  Class:  Agricultura\n",
      "The dataframe was written to file!\n",
      "\n",
      "\n",
      "Processing metrics for:  Ratio\n",
      "  Class:  Pasto\n",
      "The dataframe was written to file!\n",
      "  Class:  Floresta\n",
      "The dataframe was written to file!\n",
      "  Class:  Agricultura\n",
      "The dataframe was written to file!\n",
      "\n",
      "\n",
      "Processing metrics for:  RGI\n",
      "  Class:  Pasto\n",
      "The dataframe was written to file!\n",
      "  Class:  Floresta\n",
      "The dataframe was written to file!\n",
      "  Class:  Agricultura\n",
      "The dataframe was written to file!\n",
      "\n",
      "\n",
      "Processing metrics for:  VH\n",
      "  Class:  Pasto\n",
      "The dataframe was written to file!\n",
      "  Class:  Floresta\n",
      "The dataframe was written to file!\n",
      "  Class:  Agricultura\n",
      "The dataframe was written to file!\n",
      "\n",
      "\n",
      "Processing metrics for:  VV\n",
      "  Class:  Pasto\n",
      "The dataframe was written to file!\n",
      "  Class:  Floresta\n",
      "The dataframe was written to file!\n",
      "  Class:  Agricultura\n",
      "The dataframe was written to file!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list_df_samples_metrics = []\n",
    "\n",
    "i = 0\n",
    "raster_id = 0\n",
    "for raster in dir_datacubes:\n",
    "    file_id = 0\n",
    "    print(\"Processing metrics for: \", filenames[raster_id])\n",
    "    \n",
    "    for shp in dir_samples:\n",
    "        print(\"  Class: \", filenames_samples[file_id])\n",
    "        df_metrics = ComputeMetrics(list_df_samples[i])\n",
    "\n",
    "        # Writing the dataframe to CSV file\n",
    "        if (write_CSV == 'YES'):\n",
    "            filename = dir_output+filenames[raster_id]+'_Samples_'+filenames_samples[file_id]+'_metrics'+'.csv'\n",
    "            \n",
    "            try:\n",
    "                #df_metrics.to_csv(filename, sep=',', index=False, encoding='utf-8')\n",
    "                df_metrics.to_csv(filename, sep=',', index=False, encoding='utf-8-sig') # using 'utf-8-sig' encoding \n",
    "                                                                                #improves efficiency to open it on Excel.\n",
    "                print(\"The dataframe was written to file!\")\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "\n",
    "        list_df_samples_metrics.append(df_metrics)\n",
    "\n",
    "        i = i + 1\n",
    "        file_id = file_id + 1\n",
    "    raster_id = raster_id + 1\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure 4 - Preparation of the files required for classification\n",
    "\n",
    "The classification method implemented in this study require the description of the samples related to all classes in an unique dataframe. Therefore, we concatenated the dataframes obtained in Procedure 3. In addition, it was necessary to include a new column to these dataframes in order to identify the class associated with each pixel/sample in the final dataframe.\n",
    "\n",
    "We performed this procedure in order to obtained three different dataframes. The first dataframe presents the time series of each sample as pixel values, while the second dataframe describes the time series of each sample using different metrics. The third dataframe presents the class identifiers associated with the samples described in the other dataframes. \n",
    "\n",
    "In the classification stage, we will use these dataframes to perform two different classifications, using the pixel values and the metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class identifiers\n",
    "\n",
    "We used the following numbers to identify the analysed classes.\n",
    "    \n",
    "    1 - Agricultura\n",
    "    2 - Floresta\n",
    "    3 - Pasto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pixel values and metrics for:  NL\n",
      "  Class:  Pasto\n",
      "  Class:  Floresta\n",
      "  Class:  Agricultura\n",
      "    The dataframes were written to file!\n",
      "\n",
      "\n",
      "Processing pixel values and metrics for:  Ratio\n",
      "  Class:  Pasto\n",
      "  Class:  Floresta\n",
      "  Class:  Agricultura\n",
      "    The dataframes were written to file!\n",
      "\n",
      "\n",
      "Processing pixel values and metrics for:  RGI\n",
      "  Class:  Pasto\n",
      "  Class:  Floresta\n",
      "  Class:  Agricultura\n",
      "    The dataframes were written to file!\n",
      "\n",
      "\n",
      "Processing pixel values and metrics for:  VH\n",
      "  Class:  Pasto\n",
      "  Class:  Floresta\n",
      "  Class:  Agricultura\n",
      "    The dataframes were written to file!\n",
      "\n",
      "\n",
      "Processing pixel values and metrics for:  VV\n",
      "  Class:  Pasto\n",
      "  Class:  Floresta\n",
      "  Class:  Agricultura\n",
      "    The dataframes were written to file!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "raster_id = 0\n",
    "for raster in dir_datacubes:\n",
    "    file_id = 0\n",
    "    print(\"Processing pixel values and metrics for: \", filenames[raster_id])\n",
    "    \n",
    "    List_metrics = []\n",
    "    List_pixelValues = []\n",
    "    List_classes = []\n",
    "    for shp in dir_samples:\n",
    "        print(\"  Class: \", filenames_samples[file_id])\n",
    "        \n",
    "        if (filenames_samples[file_id] == 'Agricultura'):\n",
    "            list_df_samples[i]['Class'] = 1\n",
    "            list_df_samples_metrics[i]['Class'] = 1\n",
    "        elif (filenames_samples[file_id] == 'Floresta'):\n",
    "            list_df_samples[i]['Class'] = 2\n",
    "            list_df_samples_metrics[i]['Class'] = 2\n",
    "        elif (filenames_samples[file_id] == 'Pasto'):\n",
    "            list_df_samples[i]['Class'] = 3\n",
    "            list_df_samples_metrics[i]['Class'] = 3\n",
    "        else: # Only to avoid errors. This condition is unlikely to be required\n",
    "            list_df_samples[i]['Class'] = 0\n",
    "            list_df_samples_metrics[i]['Class'] = 0\n",
    "        \n",
    "        List_pixelValues.append(list_df_samples[i])\n",
    "        List_metrics.append(list_df_samples_metrics[i])\n",
    "        List_classes.append(list_df_samples_metrics[i][['Class']])\n",
    "        \n",
    "        i = i + 1\n",
    "        file_id = file_id + 1\n",
    "        \n",
    "    df_Samples_all_pixelValues = pd.concat(List_pixelValues, ignore_index=True)\n",
    "    df_Samples_all_metrics = pd.concat(List_metrics, ignore_index=True)\n",
    "    df_Samples_all_classes = pd.concat(List_classes, ignore_index=True)\n",
    "\n",
    "    # Writing the dataframe to CSV file\n",
    "    if (write_CSV == 'YES'):\n",
    "        filename_pValues = dir_output+filenames[raster_id]+'_AllSamples_pValues'+'.csv'\n",
    "        filename_metrics = dir_output+filenames[raster_id]+'_AllSamples_metrics'+'.csv'\n",
    "        filename_classes = dir_output+filenames[raster_id]+'_AllSamples_classes'+'.csv'\n",
    "\n",
    "        try:\n",
    "            df_Samples_all_pixelValues.to_csv(filename_pValues, sep=',', index=False, encoding='utf-8-sig')\n",
    "            df_Samples_all_metrics.to_csv(filename_metrics, sep=',', index=False, encoding='utf-8-sig')\n",
    "            df_Samples_all_classes.to_csv(filename_classes, sep=',', index=False, encoding='utf-8-sig')\n",
    "            print(\"    The dataframes were written to file!\")\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "    \n",
    "    raster_id = raster_id + 1\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
